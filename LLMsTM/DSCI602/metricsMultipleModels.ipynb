{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkWZdyMVExE_"
   },
   "source": [
    "Finding the classification metrics for each of the models tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4MYEVgnB_hU3"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def load_json_to_df(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Flatten llm_output.label\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "    # Rename columns for convenience\n",
    "    df = df.rename(columns={\n",
    "        \"label\": \"human_label\",\n",
    "        \"llm_output.label\": \"model_pred\"\n",
    "    })\n",
    "    \n",
    "    # Fix indexing: model predictions are 0-indexed (0-6), human labels are 1-indexed (1-7)\n",
    "    # Add 1 to model predictions to align with human labels\n",
    "    df[\"model_pred\"] = df[\"model_pred\"] + 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def compute_metrics(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_count = df_clean[\"model_pred\"].isna().sum()\n",
    "    total_count = len(df_clean)\n",
    "    \n",
    "    print(f\"\\n===== Data Summary =====\")\n",
    "    print(f\"Total samples: {total_count}\")\n",
    "    print(f\"NaN predictions: {nan_count} ({nan_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f\"\\nRemoving {nan_count} samples with NaN predictions for metrics calculation...\")\n",
    "        df_clean = df_clean.dropna(subset=[\"model_pred\"])\n",
    "    \n",
    "    # Also check for NaN in true labels\n",
    "    nan_true = df_clean[\"human_label\"].isna().sum()\n",
    "    if nan_true > 0:\n",
    "        print(f\"Removing {nan_true} samples with NaN true labels...\")\n",
    "        df_clean = df_clean.dropna(subset=[\"human_label\"])\n",
    "    \n",
    "    if len(df_clean) == 0:\n",
    "        print(\"\\n ERROR: No valid samples remaining after removing NaN values!\")\n",
    "        return\n",
    "    \n",
    "    y_true = df_clean[\"human_label\"]\n",
    "    y_pred = df_clean[\"model_pred\"]\n",
    "    \n",
    "    print(f\"\\nValid samples for evaluation: {len(df_clean)}\")\n",
    "    print(f\"Unique classes in true labels: {sorted(y_true.unique())}\")\n",
    "    print(f\"Unique classes in predictions: {sorted(y_pred.unique())}\")\n",
    "    \n",
    "    print(\"\\n===== Classification Report =====\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data(df, name=\"Dataset\"):\n",
    "    \"\"\"Inspect the dataframe for potential issues\"\"\"\n",
    "    print(f\"\\n===== {name} Inspection =====\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    print(f\"\\nNaN counts:\")\n",
    "    nan_counts = df.isna().sum()\n",
    "    for col in nan_counts[nan_counts > 0].index:\n",
    "        print(f\"  {col}: {nan_counts[col]} ({nan_counts[col]/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Show sample with NaN predictions if any\n",
    "    if \"model_pred\" in df.columns and df[\"model_pred\"].isna().any():\n",
    "        print(f\"\\nSample rows with NaN predictions:\")\n",
    "        print(df[df[\"model_pred\"].isna()].head())\n",
    "    \n",
    "    # Show class distribution\n",
    "    if \"human_label\" in df.columns:\n",
    "        print(f\"\\nClass distribution (human_label):\")\n",
    "        print(df[\"human_label\"].value_counts().sort_index())\n",
    "    \n",
    "    if \"model_pred\" in df.columns:\n",
    "        print(f\"\\nClass distribution (model_pred):\")\n",
    "        print(df[\"model_pred\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7674RuzFV7a"
   },
   "source": [
    "## Model Evaluations\n",
    "\n",
    "### Claude Haiku (Zero-shot and Few-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv-fe_VYFZbv",
    "outputId": "beeabfc0-52ca-4153-88c4-3576423d0bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Data Summary =====\n",
      "Total samples: 10\n",
      "NaN predictions: 0 (0.00%)\n",
      "\n",
      "Valid samples for evaluation: 10\n",
      "Unique classes in true labels: [1, 3, 5, 6]\n",
      "Unique classes in predictions: [1, 3, 4, 6]\n",
      "\n",
      "===== Classification Report =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3333    1.0000    0.5000         1\n",
      "           3     1.0000    1.0000    1.0000         1\n",
      "           4     0.0000    0.0000    0.0000         0\n",
      "           5     0.0000    0.0000    0.0000         4\n",
      "           6     0.6667    0.5000    0.5714         4\n",
      "\n",
      "    accuracy                         0.4000        10\n",
      "   macro avg     0.4000    0.5000    0.4143        10\n",
      "weighted avg     0.4000    0.4000    0.3786        10\n",
      "\n",
      "\n",
      "===== Data Summary =====\n",
      "Total samples: 10\n",
      "NaN predictions: 0 (0.00%)\n",
      "\n",
      "Valid samples for evaluation: 10\n",
      "Unique classes in true labels: [1, 3, 5, 6]\n",
      "Unique classes in predictions: [1, 3, 4, 6]\n",
      "\n",
      "===== Classification Report =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3333    1.0000    0.5000         1\n",
      "           3     1.0000    1.0000    1.0000         1\n",
      "           4     0.0000    0.0000    0.0000         0\n",
      "           5     0.0000    0.0000    0.0000         4\n",
      "           6     0.6667    0.5000    0.5714         4\n",
      "\n",
      "    accuracy                         0.4000        10\n",
      "   macro avg     0.4000    0.5000    0.4143        10\n",
      "weighted avg     0.4000    0.4000    0.3786        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "claude_zero_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/Zero-Shot/claude-haiku-4-5-20251001_outputs.json\"\n",
    "claude_few_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/Few-Shot/claude-haiku-4-5-20251001_outputs.json\"\n",
    "\n",
    "\n",
    "df_haiku_zeroshot = load_json_to_df(claude_zero_path)\n",
    "compute_metrics(df_haiku_zeroshot)\n",
    "\n",
    "df_haiku_fewshot = load_json_to_df(claude_few_path)\n",
    "compute_metrics(df_haiku_fewshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPNqrhoiGYe0"
   },
   "source": [
    "**Note:** The Claude Haiku files contain only 10 samples. Need to check with Soumyajit.\n",
    "\n",
    "---\n",
    "\n",
    "### GPT-4o-mini (Zero-shot and Few-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "id": "fkextZfmGENl",
    "outputId": "926e7ac7-9ce9-4d68-9ad1-e11c3b71eb71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-mini Zero-shot\n",
      "\n",
      "===== Data Summary =====\n",
      "Total samples: 2707\n",
      "NaN predictions: 0 (0.00%)\n",
      "\n",
      "Valid samples for evaluation: 2707\n",
      "Unique classes in true labels: [1, 2, 3, 4, 5, 6, 7]\n",
      "Unique classes in predictions: [1, 2, 3, 4, 5, 6, 7]\n",
      "\n",
      "===== Classification Report =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.2533    0.4113    0.3135       231\n",
      "           2     0.1828    0.8793    0.3027        58\n",
      "           3     0.4435    0.9351    0.6017       231\n",
      "           4     0.4607    0.6241    0.5301       141\n",
      "           5     0.5484    0.2853    0.3753       715\n",
      "           6     0.7910    0.4977    0.6110      1065\n",
      "           7     0.5165    0.6466    0.5743       266\n",
      "\n",
      "    accuracy                         0.5009      2707\n",
      "   macro avg     0.4566    0.6113    0.4727      2707\n",
      "weighted avg     0.5942    0.5009    0.5081      2707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_mini_zero_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/Zero-Shot/gpt-4o-mini_outputs.json\"\n",
    "gpt_mini_few_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/Few-Shot/gpt-4o-mini_outputs.json\"\n",
    "\n",
    "# Zero-shot evaluation\n",
    "print(\"GPT-4o-mini Zero-shot\")\n",
    "df_mini_zeroshot = load_json_to_df(gpt_mini_zero_path)\n",
    "compute_metrics(df_mini_zeroshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o-mini Few-shot\n",
      "\n",
      "===== Data Summary =====\n",
      "Total samples: 2707\n",
      "NaN predictions: 1 (0.04%)\n",
      "\n",
      "Removing 1 samples with NaN predictions for metrics calculation...\n",
      "\n",
      "Valid samples for evaluation: 2706\n",
      "Unique classes in true labels: [1, 2, 3, 4, 5, 6, 7]\n",
      "Unique classes in predictions: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "\n",
      "===== Classification Report =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0     0.2651    0.2857    0.2750       231\n",
      "         2.0     0.1672    0.9138    0.2827        58\n",
      "         3.0     0.4136    0.8182    0.5494       231\n",
      "         4.0     0.1370    0.0714    0.0939       140\n",
      "         5.0     0.3659    0.3147    0.3383       715\n",
      "         6.0     0.7333    0.3718    0.4935      1065\n",
      "         7.0     0.3782    0.6128    0.4677       266\n",
      "         8.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.4072      2706\n",
      "   macro avg     0.3075    0.4236    0.3126      2706\n",
      "weighted avg     0.4911    0.4072    0.4109      2706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Few-shot evaluation\n",
    "print(\"GPT-4o-mini Few-shot\")\n",
    "\n",
    "df_mini_fewshot = load_json_to_df(gpt_mini_few_path)\n",
    "# # Inspect the data first to understand the NaN issue\n",
    "# inspect_data(df_mini_fewshot, name=\"GPT-4o-mini Few-shot\")\n",
    "compute_metrics(df_mini_fewshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it instead for gpt-oss to see if that works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSS Zero-shot\n",
      "Loaded 2707 GPT-OSS elements\n",
      "Nulls in human_label: 0\n",
      "Nulls in oss_pred: 1\n",
      "OSS Accuracy: 0.520\n",
      "OSS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.47      0.35       231\n",
      "           2       0.25      0.78      0.38        58\n",
      "           3       0.50      0.86      0.63       231\n",
      "           4       0.43      0.60      0.50       141\n",
      "           5       0.47      0.44      0.45       715\n",
      "           6       0.78      0.49      0.60      1064\n",
      "           7       0.65      0.50      0.57       266\n",
      "\n",
      "    accuracy                           0.52      2706\n",
      "   macro avg       0.48      0.59      0.50      2706\n",
      "weighted avg       0.59      0.52      0.53      2706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_gpt_oss_json_to_df(json_path):\n",
    "    \"\"\"\n",
    "    Load GPT-OSS outputs into a dataframe.\n",
    "    - 'human_label' = int from 'llm_output'\n",
    "    - 'oss_pred' = int parsed from 'llm_output_gpt'['content'] (JSON string or dict with 'label')\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    elements = []\n",
    "    for entry in data:\n",
    "        d = {}\n",
    "        d[\"human_label\"] = None\n",
    "        d[\"oss_pred\"] = None\n",
    "        # Human-labeled value\n",
    "        if \"llm_output\" in entry:\n",
    "            d[\"human_label\"] = entry[\"llm_output\"]\n",
    "        # Model prediction: \"llm_output_gpt\" may contain a \"content\" like '{\"label\": 6}'\n",
    "        oss_pred = None\n",
    "        gpt_block = entry.get(\"llm_output_gpt\", None)\n",
    "        if gpt_block:\n",
    "            content = gpt_block.get(\"content\", None)\n",
    "            if content:\n",
    "                try:\n",
    "                    # Try parsing prediction as JSON first\n",
    "                    oss_pred = json.loads(content).get(\"label\", None)\n",
    "                except Exception:\n",
    "                    # Fallback: extract integer label via regex\n",
    "                    match = re.search(r'\"label\"\\s*:\\s*(\\d+)', str(content))\n",
    "                    if match:\n",
    "                        oss_pred = int(match.group(1))\n",
    "                if oss_pred is not None:\n",
    "                    d[\"oss_pred\"] = int(oss_pred)\n",
    "        elements.append(d)\n",
    "    df = pd.DataFrame(elements)\n",
    "    print(f\"Loaded {len(df)} GPT-OSS elements\")\n",
    "    print(\"Nulls in human_label:\", df['human_label'].isnull().sum())\n",
    "    print(\"Nulls in oss_pred:\", df['oss_pred'].isnull().sum())\n",
    "    return df\n",
    "\n",
    "def compute_oss_metrics(df):\n",
    "    \"\"\"\n",
    "    Compute accuracy and classification report for GPT-OSS model outputs.\n",
    "    Expects df with 'human_label' and 'oss_pred' columns.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    df_eval = df.dropna(subset=['human_label', 'oss_pred']).copy()\n",
    "    y_true = df_eval[\"human_label\"].astype(int)\n",
    "    y_pred = df_eval[\"oss_pred\"].astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"OSS Accuracy: {acc:.3f}\")\n",
    "    print(\"OSS Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0.0))\n",
    "\n",
    "# OSS Zero-shot path\n",
    "gpt_oss_zero_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/gpt-oss_20b_outputs.json\"\n",
    "gpt_oss_few_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/gpt-oss_20b_outputs_few.json\"\n",
    "\n",
    "print(\"OSS Zero-shot\")\n",
    "df_oss_zeroshot = load_gpt_oss_json_to_df(gpt_oss_zero_path)\n",
    "compute_oss_metrics(df_oss_zeroshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSS Few-shot\n",
      "Loaded 2707 GPT-OSS elements\n",
      "Nulls in human_label: 0\n",
      "Nulls in oss_pred: 0\n",
      "OSS Accuracy: 0.504\n",
      "OSS Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.42      0.34       231\n",
      "           2       0.23      0.83      0.37        58\n",
      "           3       0.49      0.89      0.63       231\n",
      "           4       0.41      0.61      0.49       141\n",
      "           5       0.46      0.45      0.46       715\n",
      "           6       0.81      0.43      0.57      1065\n",
      "           7       0.58      0.53      0.55       266\n",
      "\n",
      "    accuracy                           0.50      2707\n",
      "   macro avg       0.47      0.59      0.48      2707\n",
      "weighted avg       0.59      0.50      0.51      2707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Few-shot\n",
    "print(\"OSS Few-shot\")\n",
    "df_oss_fewshot = load_gpt_oss_json_to_df(gpt_oss_few_path)\n",
    "\n",
    "compute_oss_metrics(df_oss_fewshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral Zero-shot\n",
      "Loaded 2707 Mistral elements\n",
      "Nulls in human_label: 0\n",
      "Nulls in mistral_pred: 2\n",
      "Mistral Accuracy: 0.280\n",
      "Mistral Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.19      0.49      0.28       231\n",
      "           2       0.11      0.93      0.20        58\n",
      "           3       0.30      0.78      0.43       231\n",
      "           4       0.22      0.60      0.32       141\n",
      "           5       0.42      0.07      0.12       715\n",
      "           6       0.61      0.14      0.23      1063\n",
      "           7       0.48      0.47      0.47       266\n",
      "\n",
      "    accuracy                           0.28      2705\n",
      "   macro avg       0.33      0.50      0.29      2705\n",
      "weighted avg       0.45      0.28      0.25      2705\n",
      "\n",
      "Mistral Few-shot\n",
      "Loaded 2707 Mistral elements\n",
      "Nulls in human_label: 0\n",
      "Nulls in mistral_pred: 1\n",
      "Mistral Accuracy: 0.258\n",
      "Mistral Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.17      0.23      0.20       231\n",
      "           2       0.08      0.97      0.15        58\n",
      "           3       0.26      0.83      0.39       231\n",
      "           4       0.23      0.60      0.33       141\n",
      "           5       0.37      0.13      0.20       715\n",
      "           6       0.64      0.13      0.21      1064\n",
      "           7       0.57      0.30      0.39       266\n",
      "\n",
      "    accuracy                           0.26      2706\n",
      "   macro avg       0.33      0.46      0.27      2706\n",
      "weighted avg       0.45      0.26      0.25      2706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Mistral 7B output loader and metrics ------------------\n",
    "\n",
    "def load_mistral_json_to_df(json_path):\n",
    "    \"\"\"\n",
    "    Load Mistral-7B outputs into a dataframe with columns:\n",
    "    'human_label' = int from 'llm_output'\n",
    "    'mistral_pred' = int parsed from 'llm_output_mistral' (which is a string, e.g. '{\\n    \"label\": 4\\n}')\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    elements = []\n",
    "    for entry in data:\n",
    "        d = {}\n",
    "        d[\"human_label\"] = None\n",
    "        d[\"mistral_pred\"] = None\n",
    "        # Human (true) label\n",
    "        if \"llm_output\" in entry:\n",
    "            d[\"human_label\"] = entry[\"llm_output\"]\n",
    "        # Pred label is usually a string like '{\"label\": 4}' or similar\n",
    "        pred_str = entry.get(\"llm_output_mistral\", None)\n",
    "        if pred_str:\n",
    "            label_pred = None\n",
    "            try:\n",
    "                # Try json.loads\n",
    "                label_pred = json.loads(pred_str).get(\"label\", None)\n",
    "            except Exception:\n",
    "                # Try regex fallback\n",
    "                match = re.search(r'\"label\"\\s*:\\s*(\\d+)', pred_str)\n",
    "                if match:\n",
    "                    label_pred = int(match.group(1))\n",
    "            if label_pred is not None:\n",
    "                d[\"mistral_pred\"] = int(label_pred)\n",
    "        elements.append(d)\n",
    "    df = pd.DataFrame(elements)\n",
    "    print(f\"Loaded {len(df)} Mistral elements\")\n",
    "    print(\"Nulls in human_label:\", df['human_label'].isnull().sum())\n",
    "    print(\"Nulls in mistral_pred:\", df['mistral_pred'].isnull().sum())\n",
    "    return df\n",
    "\n",
    "def compute_mistral_metrics(df):\n",
    "    \"\"\"\n",
    "    Compute accuracy and classification report for Mistral model outputs.\n",
    "    Assumes df has 'human_label' and 'mistral_pred' columns.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    df_eval = df.dropna(subset=['human_label', 'mistral_pred']).copy()\n",
    "    y_true = df_eval[\"human_label\"].astype(int)\n",
    "    y_pred = df_eval[\"mistral_pred\"].astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Mistral Accuracy: {acc:.3f}\")\n",
    "    print(\"Mistral Classification Report:\")\n",
    "    # Use zero_division for safety\n",
    "    print(classification_report(y_true, y_pred, zero_division=0.0))\n",
    "\n",
    "# Paths for Mistral outputs\n",
    "mistral_zero_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/mistral_7b_outputs.json\"\n",
    "mistral_few_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/mistral_7b_outputs_few.json\"\n",
    "\n",
    "# Zero-shot\n",
    "print(\"Mistral Zero-shot\")\n",
    "df_mistral_zeroshot = load_mistral_json_to_df(mistral_zero_path)\n",
    "compute_mistral_metrics(df_mistral_zeroshot)\n",
    "\n",
    "# Few-shot\n",
    "print(\"Mistral Few-shot\")\n",
    "df_mistral_fewshot = load_mistral_json_to_df(mistral_few_path)\n",
    "compute_mistral_metrics(df_mistral_fewshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama Zero-shot\n",
      "Loaded 2707 Llama elements\n",
      "Nulls in human_label: 0\n",
      "Nulls in llama_pred: 1\n",
      "Llama Accuracy: 0.373\n",
      "Llama Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.45      0.36       231\n",
      "           2       0.21      0.84      0.34        58\n",
      "           3       0.28      0.88      0.43       231\n",
      "           4       0.20      0.69      0.31       141\n",
      "           5       0.51      0.16      0.24       714\n",
      "           6       0.72      0.29      0.41      1065\n",
      "           7       0.49      0.51      0.50       266\n",
      "\n",
      "    accuracy                           0.37      2706\n",
      "   macro avg       0.39      0.55      0.37      2706\n",
      "weighted avg       0.53      0.37      0.37      2706\n",
      "\n",
      "Llama Few-shot\n",
      "Loaded 2707 Llama elements\n",
      "Nulls in human_label: 0\n",
      "Nulls in llama_pred: 38\n",
      "Llama Accuracy: 0.446\n",
      "Llama Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.31      0.48      0.37       229\n",
      "           2       0.25      0.67      0.37        57\n",
      "           3       0.32      0.88      0.46       231\n",
      "           4       0.32      0.69      0.43       141\n",
      "           5       0.50      0.24      0.32       712\n",
      "           6       0.70      0.44      0.54      1041\n",
      "           7       0.53      0.44      0.48       258\n",
      "\n",
      "    accuracy                           0.45      2669\n",
      "   macro avg       0.42      0.55      0.43      2669\n",
      "weighted avg       0.53      0.45      0.45      2669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_llama_json_to_df(json_path):\n",
    "    \"\"\"\n",
    "    Load Llama 3.1 8B outputs into a dataframe with columns:\n",
    "    'Serial_Number' = int from 'Serial_Number' in the json (if exists)\n",
    "    'human_label' = int from 'llm_output'\n",
    "    'llama_pred' = int parsed from 'llm_output_llama' (which is a string, e.g. '{\\n    \"label\": 5\\n}')\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    elements = []\n",
    "    for entry in data:\n",
    "        d = {}\n",
    "        # Add Serial_Number if it exists in the entry, else None\n",
    "        d[\"Serial_Number\"] = entry.get(\"Serial_Number\", None)\n",
    "        d[\"human_label\"] = None\n",
    "        d[\"llama_pred\"] = None\n",
    "        # Human (true) label\n",
    "        if \"llm_output\" in entry:\n",
    "            d[\"human_label\"] = entry[\"llm_output\"]\n",
    "        # Pred label is usually a string like '{\"label\": 4}' or similar\n",
    "        pred_str = entry.get(\"llm_output_llama\", None)\n",
    "        if pred_str:\n",
    "            label_pred = None\n",
    "            try:\n",
    "                label_pred = json.loads(pred_str).get(\"label\", None)\n",
    "            except Exception:\n",
    "                match = re.search(r'\"label\"\\s*:\\s*(\\d+)', pred_str)\n",
    "                if match:\n",
    "                    label_pred = int(match.group(1))\n",
    "            if label_pred is not None:\n",
    "                d[\"llama_pred\"] = int(label_pred)\n",
    "        elements.append(d)\n",
    "    df = pd.DataFrame(elements)\n",
    "    print(f\"Loaded {len(df)} Llama elements\")\n",
    "    print(\"Nulls in human_label:\", df['human_label'].isnull().sum())\n",
    "    print(\"Nulls in llama_pred:\", df['llama_pred'].isnull().sum())\n",
    "    return df\n",
    "\n",
    "def compute_llama_metrics(df):\n",
    "    \"\"\"\n",
    "    Compute accuracy and classification report for Llama model outputs.\n",
    "    Assumes df has 'human_label' and 'llama_pred' columns.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    df_eval = df.dropna(subset=['human_label', 'llama_pred']).copy()\n",
    "    y_true = df_eval[\"human_label\"].astype(int)\n",
    "    y_pred = df_eval[\"llama_pred\"].astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Llama Accuracy: {acc:.3f}\")\n",
    "    print(\"Llama Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0.0))\n",
    "\n",
    "\n",
    "llama_few_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/llama3.1_8b_outputs_few.json\"\n",
    "llama_zero_path = \"/Users/hetavpatel/Desktop/Data Science/Grad DS Work/DSCI 601 Applied Data Science/old_repos/NLPPunePorsche/LLMsTM/DSCI602/outputs-puneporshe/llama3.1_8b_outputs.json\"\n",
    "\n",
    "print(\"Llama Zero-shot\")\n",
    "df_llama_zeroshot = load_llama_json_to_df(llama_zero_path)\n",
    "compute_llama_metrics(df_llama_zeroshot)\n",
    "\n",
    "print(\"Llama Few-shot\")\n",
    "df_llama_fewshot = load_llama_json_to_df(llama_few_path)\n",
    "compute_llama_metrics(df_llama_fewshot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Serial Numbers with null llama_pred in Zero-shot file:\n",
      "[2864]\n",
      "\n",
      "Serial Numbers with null llama_pred in Few-shot file:\n",
      "[219, 1697, 3047, 1758, 2605, 1107, 1991, 921, 1767, 1310, 835, 2667, 2639, 918, 1642, 2864, 243, 1331, 904, 2118, 1339, 2359, 773, 2972, 395, 2066, 3042, 1604, 2487, 2405, 2057, 2236, 1955, 498, 2198, 1970, 2503, 1722]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Identify the rows where 'llama_pred' is null in both zero-shot and few-shot llama outputs,\n",
    "# and print the corresponding Serial_Number(s) (from the JSON)\n",
    "\n",
    "print(\"\\nSerial Numbers with null llama_pred in Zero-shot file:\")\n",
    "null_zeroshot = df_llama_zeroshot[df_llama_zeroshot['llama_pred'].isnull()]\n",
    "if null_zeroshot.empty:\n",
    "    print(\"None\")\n",
    "else:\n",
    "    print(null_zeroshot['Serial_Number'].tolist())\n",
    "\n",
    "print(\"\\nSerial Numbers with null llama_pred in Few-shot file:\")\n",
    "null_fewshot = df_llama_fewshot[df_llama_fewshot['llama_pred'].isnull()]\n",
    "if null_fewshot.empty:\n",
    "    print(\"None\")\n",
    "else:\n",
    "    print(null_fewshot['Serial_Number'].tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
